import json
import os
import time
import uuid
from copy import deepcopy
import csv
import sys
import pathlib

import boto3
import dash
import dash_core_components as dcc
import dash_html_components as html
import dash_reusable_components as drc
import requests
from dash.dependencies import Input, Output, State
from flask_caching import Cache

DEBUG = True
LOCAL = False
APP_PATH = str(pathlib.Path(__file__).parent.resolve())

app = dash.Dash(__name__)
server = app.server

if "BUCKET_NAME" in os.environ:
    # Change caching to redis if hosted on dds
    cache_config = {
        "CACHE_TYPE": "redis",
        "CACHE_REDIS_URL": os.environ["REDIS_URL"],
        "CACHE_THRESHOLD": 400,
    }
# Local Conditions
else:
    LOCAL = True
    # Caching with filesystem when served locally
    cache_config = {
        "CACHE_TYPE": "filesystem",
        "CACHE_DIR": os.path.join(APP_PATH, "data"),
    }

# S3 Client. It is used to store user images. The bucket name
# is stored inside the utils file, the key is
# the session id generated by uuid

access_key_id = os.environ.get("ACCESS_KEY_ID")
secret_access_key = os.environ.get("SECRET_ACCESS_KEY")
bucket_name = os.environ.get("BUCKET_NAME")

# Empty cache directory before running the app
folder = os.path.join(APP_PATH, "data")
for the_file in os.listdir(folder):
    file_path = os.path.join(folder, the_file)
    try:
        if os.path.isfile(file_path):
            os.unlink(file_path)
    except Exception as e:
        print(e)

# If local, image data is stored locally in image_string.csv
if LOCAL:
    f = open("image_string.csv", "w+")
    f.close()

    # Store images are very long strings, so allowed csv
    # reading length is increased to its maximum allowed value
    maxInt = sys.maxsize
    while True:
        # decrease the maxInt value by factor 10
        # as long as the OverflowError occurs.
        try:
            csv.field_size_limit(maxInt)
            break
        except OverflowError:
            maxInt = int(maxInt / 10)

if not LOCAL:
    s3 = boto3.client(
        "s3",
        endpoint_url="https://storage.googleapis.com",
        aws_access_key_id=access_key_id,
        aws_secret_access_key=secret_access_key,
    )

# Caching
cache = Cache()
cache.init_app(app.server, config=cache_config)


# Store key value value (session_id, stringed_image)
def store_image_string(string_image, key_name):
    if DEBUG:
        print(key_name)
    # If local, the string is stored in image_string.csv
    if LOCAL:
        with open("image_string.csv", mode="w+") as image_file:
            image_writer = csv.DictWriter(image_file, fieldnames=["key", "image"])
            image_writer.writeheader()
            image_writer.writerow(dict(key=key_name, image=string_image))
    # Generate the POST attributes
    else:
        post = s3.generate_presigned_post(Bucket=bucket_name, Key=key_name)

        files = {"file": string_image}
        # Post the string file using requests
        requests.post(post["url"], data=post["fields"], files=files)

def serve_layout():
	# Generates a session ID
    session_id = str(uuid.uuid4())

    # Post the image to the right key, inside the bucket named after the
    # session ID
    store_image_string(utils.IMAGE_STRING_PLACEHOLDER, session_id)

    return html.Div(
    	id="root",
    	children=[
    		html.Div(session_id,id="session-id",),
    		html.Div(
    			id="app-container",
    			children=[
    				html.Div(
                    id="div-interactive-image",
        				children=[
    	    				utils.GRAPH_PLACEHOLDER,
    	    				html.Div(
    	    					id="div-storage",
    	    					children=utils.STORAGE_PLACEHOLDER,
    	    						),
        						],
        					),
        				],
    			),
    		html.Div(
    			id="upload",
    			children=[
    				"Drag and Drop Picture",
    				html.A(children="Select a photo"),
    				],
    			),
    		],
    	)
    


